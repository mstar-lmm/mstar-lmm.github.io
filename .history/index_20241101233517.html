<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Multimodal Self-Evolving Training">
  <meta name="keywords" content="MSTAR, M-STAR, M-STaR">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title> M-STAR: Diving into Self-Evolving Training for Multimodal Reasoning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
/Users/panlu/Library/Mobile Documents/com~apple~CloudDocs/ImageMath/visual-mathqa-server/data_final/images
    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link rel="icon" href="./static/images/mstar-logo.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <!-- <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bulma.min.css" rel="stylesheet">
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script> -->
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>  
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <!-- <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a> -->
      <!-- @PAN TODO: consider adding links? -->
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/hkust-nlp/deita">
            <b>Deita</b> <p style="font-size:18px; display: inline; margin-left: 5px;">üî•</p>
          </a>
          <a class="navbar-item" href="https://cevalbenchmark.com/">
            <b>C-Eval</b> <p style="font-size:18px; display: inline; margin-left: 5px;">üî•</p>
          </a>
          <a class="navbar-item" href="https://hkust-nlp.github.io/agentboard/">
            <b>AgentBoard</b> <p style="font-size:18px; display: inline; margin-left: 5px;">üî•</p>
          </a>
          <a class="navbar-item" href="https://github.com/hkust-nlp/llm-compression-intelligence">
            <b>LLM Compression Intelligence</b> <p style="font-size:18px; display: inline; margin-left: 5px;">üî•</p>
          </a>
          <a class="navbar-item" href="https://lupantech.github.io/inter-gps/">
            Inter-GPS
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="static/images/mstar-logo.png" style="width: 400px;vertical-align: middle" alt="Logo"/>
          <h1 class="title is-1 publication-title is-bold">
            <img src="static/images/star.png" style="width:1.5em;vertical-align: middle" alt="Logo"/><span class="mathvista" style="vertical-align: middle">M-STAR</span>
          </h1>
          <h2 class="subtitle is-3 publication-subtitle">
            Diving into Self-Evolving Training for Multimodal Reasoning
            <!-- <br> -->
            <!-- with GPT-4V, Bard, and Other Large Multimodal Models -->
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://vpeterv.github.io/">Wei Liu*</a><sup style="color:#6fbf73;">1</sup>,</span>
            <span class="author-block">
              <a href="https://lockon-n.github.io/">Junlong Li*</a><sup style="color:#ed4b82;">2</sup>,</span>
            <span class="author-block">
              <a href="https://xiwen1995.github.io/">Xiwen Zhang</a><sup style="color:#9c27b0;">3</sup>,
            </span>
            <span class="author-block">
              <a href="https://koalazf99.github.io/">Fan Zhou</a><sup style="color:#ed4b82">2</sup>,
            </span>
            <span class="author-block">
              <a href="https://ych133.github.io/">Yu Cheng</a><sup style="color:#ffac33">4</sup>,
            </span>
            <span class="author-block">
              <a href="https://jxhe.github.io/">Junxian He</a><sup style="color:#6fbf73;">1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup style="color:#6fbf73;">1</sup> The Hong Kong University of Science and Technology,</span>
            <span class="author-block"><sup style="color:#ed4b82">2</sup>Shanghai Jiao Tong University,</span><br>
            <span class="author-block"><sup style="color:#9c27b0">3</sup>Helixon Research,</span>
            <span class="author-block"><sup style="color:#ffac33">4</sup>The Chinese University of Hong Kong</span><br>
          </div>
          
          <div class="is-size-5 publication-authors">
          * Equal contributions
          </div>
        
          <!-- <section> -->
            <!-- <div class="section" id="org-banners" style="display:fle">
              <a href="https://www.ucla.edu/" target="_blank" rel="external">
                  <img class="center-block org-banner" src="static/images/ucla.png" style="height:3em">
              </a>
              <a href="https://www.washington.edu/" target="blank" class="ext-link">
                  <img class="center-block org-banner" src="static/images/uw.png" style="height:3em">
              </a>
              <a href="https://www.microsoft.com/en-us/research/" target="_blank" rel="external">
                  <img class="center-block org-banner" src="static/images/microsoft.png" style="height:3em">
              </a>
            </div> -->
          <!-- </section> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <!-- @PAN TODO: change links -->
                <a href="https://arxiv.org/pdf/2310.02255.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/hkust-nlp/mstar"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/collections/mstar"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ü§ó</p>
                      <!-- üîó -->
                  </span>
                  <span>Resources</span>
                </a>
              </span>
              <!-- Twitter Link. -->
              <span class="link-block">
                <a href="https://twitter.com/lupantech/status/1717313355780964608"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <!-- üíªüîó -->
                      <p style="font-size:18px">üåê</p>
                  </span>
                  <span>Twitter</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">Introduction</h1>
        <div class="content has-text-justified">
        <p>
          <img src="static/images/star.png" style="width:2.0em;vertical-align: middle" alt="Logo"/>M-STAR (short for <b>M</b>ultimodal <b>S</b>elf-evolving <b>T</b>r<b>A</b>ining for <b>R</b>easoning) is a project aimed at facilitating <b>multimodal reasoning</b> via <b>self-evolving training</b>.
        </p>
        <p>
          In <img src="static/images/star.png" style="width:2.0em;vertical-align: middle" alt="Logo"/>M-STAR, we aim to answer the following questions:
          <ul>
            <li>Can we enhance <b>Multimodal Reasoning</b> through <b>Self-Evolving Training</b>?</li>
            <li>How can we <b>comprehensively understand</b> each <b>factor</b> of self-evolving training and design an optimal recipe for multimodal reasoning?</li>
            <li>What insights can we gain from the training <b>Dynamics of Self-Evolution</b>, and how can these insights inform better self-evolving training for multimodal reasoning?</li>
          </ul>
          And we release the corresponding code and resources to facilitate future research:
          <ul>
            <li><a href="https://github.com/hkust-nlp/mstar" target="_blank">M-STAR Framework: </a>A framework for Self-Evolving Training of Large Multimodal Models (LMMs) including <b>Generation</b>, <b>Training</b>, and <b>Rewarding</b>.</li>
            <li><a href="https://huggingface.co/collections/mstar" target="_blank">M-STAR Resources: </a><b>M-STAR Models</b>, <b>CoT Dataset</b>, and <b>Multimodal Process Reward Model (MPRM) Training Dataset</b>.</li>
          </ul>
        </p>
        <!-- <p>
          Reasoning ability is essential for <b>Large Multimodal Models (LMMs)</b>. In the absence of multimodal chain-of-thought annotated data, <b>self-evolving training</b>, where the model learns from its own outputs, has emerged as an effective and scalable approach for enhancing reasoning abilities. 
        </p>
        <p>
          Despite its growing usage, a comprehensive understanding of self-evolving training, particularly in the context of multimodal reasoning, remains limited. In this paper, we delve into the intricacies of <b>self-evolving training for multimodal reasoning</b>, pinpointing three key factors: <b>Training Method</b>, <b>Reward Model</b>, and <b>Prompt Variation</b>. We systematically examine each factor and explore how various configurations affect the training's effectiveness. Our analysis leads to a set of best practices for each factor, aimed at optimizing multimodal reasoning.
        </p>
        <p>
          Furthermore, we explore the <b>Self-Evolution Dynamics</b> during training and the impact of automatic balancing mechanisms in boosting performance. After all the investigations, we present a final recipe for self-evolving training in multimodal reasoning, encapsulating these design choices into a framework we call <img src="static/images/star.png" style="width:2.0em;vertical-align: middle" alt="Logo"/><b>M-STAR</b> (<b>M</b>ultimodal <b>S</b>elf-evolving <b>T</b>r<b>A</b>ining for <b>R</b>easoning), built on MiniCPM-V 2.5. 
          <img src="static/images/star.png" style="width:2.0em;vertical-align: middle" alt="Logo"/>M-STAR achieves 59.5% accuracy on MathVista, surpassing the pre-evolved model by 6.9% absolutely without using additional human annotations. 
          We believe this study fills a significant gap in the understanding of self-evolving training for multimodal reasoning and offers a robust framework for future research. Our policy and reward models, as well as the collected data, is released to facilitate further investigation in multimodal reasoning.
        </p> -->
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

<!-- DATASET SECTION -->
<!-- <section class="hero is-light is-small"> -->
  <!-- <div class="hero-body has-text-centered"> -->
    <!-- <h1 class="title is-1 mathvista"><img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
  <!-- <h1 class="title is-1 mathvista">
    <img src="static/images/mathvista.png" style="width:1em;vertical-align: middle" alt="Logo"/>
    <span class="mathvista" style="vertical-align: middle">MathVista Dataset</span>
  </h1> -->
  <!-- </div> -->
<!-- </section> -->

<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 mathvista">Overview</h1>
  </div>
</section>

<!-- <section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p> -->
            
<section class="section">
  <div class="container has-text-centered">
    <img src="static/images/main_figure.png" alt="main_figure" style="width: 60%; margin: auto;"/>
  </div>
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"><span style="color:rgb(226, 81, 74)">Exploration</span> & <span style="color:rgb(84, 133, 237)">Exploitation</span></h2>
        <div class="content has-text-justified">
          <p>
            Self-Evolving Training for Multimodal Reasoning can be considered as a loop about: <span style="color:rgb(226, 81, 74)">Exploration</span> and <span style="color:rgb(84, 133, 237)">Exploitation</span>. 
            This loop operates as a process where the model generates responses (exploration) to various question-image pairs, and learns to maximize the expected reward (exploitation). 
            Through this self-evolution cycle, the model continually refines its performance. This approach enables the model to improve its reasoning abilities without relying on extensive human-annotated data, 
            making it more efficient and scalable for multimodal reasoning.
          </p>
        </div>

        <h2 class="title is-3">Self-Evolving Design Components</h2>
        <div class="content has-text-justified">
          <p>
            To comprehensively understand this loop, we delve into the intricacies of self-evolving training for multimodal reasoning, 
            pinpointing three key factors: <span style="color:rgb(226, 81, 74)">Training Method</span>, <span style="color:rgb(226, 81, 74)">Reward Model</span>, and <span style="color:rgb(226, 81, 74)">Prompt Variation</span>.
            We systematically examine each factor and explore how various configurations affect the training's effectiveness. Our analysis leads to a set of best practices for each factor, aimed at optimizing multimodal reasoning.
          </p>
        </div>

        <h2 class="title is-3"><b>Self-Evolution Dynamics</b></h2>
        <div class="content has-text-justified">
          <p>
            Furthermore, we explore the Self-Evolution Dynamics during training and the impact of automatic balancing mechanisms in boosting performance. After all the investigations, 
            we present a final recipe considering both exploration and exploitation for self-evolving training in multimodal reasoning, encapsulating these design choices into M-STAR framework. 
            M-STAR achieves 59.5% accuracy on MathVista, surpassing the pre-evolved model by 6.9% absolutely without using additional human annotations. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- RESULTS SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 mathvista">Diving into Self-Evolving Design Components</h1>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Warm-Up Phase</h2>
        <div class="content has-text-justified">
            <div class="content has-text-centered">
              <img src="static/images/cot_instruction.png" alt="Warm-Up Phase" width="80%"/>
              <img src="static/images/cot_data.png" alt="Warm-Up Phase" width="80%"/>
          </div>
          <p>
            In the realm of multimodal training, Chain-of-Thought (CoT) data is notably scarce, which poses challenges in the development of models capable of generating intermediate reasoning steps. 
            The Warm-Up Phase in our project represents the initial step before self-evolving training, designed to establish a foundational policy model. 
            During this phase, the model is prompted to generate reasoning steps for each input triplet consisting of a question, an image, and an answer. 
            By filtering the responses based on answer accuracy, we conduct warmup training for the policy model, enabling it to begin generating coherent CoT responses. 
            This preparatory phase is crucial in equipping the policy model with the capability to produce intermediate reasoning steps, forming a stepping stone toward more advanced, self-evolving multimodal training.
          </p>
        </div>

        <h2 class="title is-3">Training Method</h2>
        <div class="content has-text-justified">
          <div class="content has-text-centered">
            <img src="static/images/training_method.png" alt="training method" width="80%"/>
          </div>
          <p>
            <strong>Key findings:</strong>
            <ul>
              <li>Optimizing the model from the last checkpoint is superior to retraining from scratch every time.</li>
              <li>Inheriting the optimizer states from the previous iteration also leads to performance improvements.</li>
              <li>Each iteration should maintain an appropriate interval to traverse the queries in training set, neither too large nor too small.</li>
              <li>All these lead to a more online Self-Evolving Training.</li>
            </ul>
          </p>
        </div>

        <h2 class="title is-3">Reward Model</h2>
        <div class="content has-text-justified">
          <div class="content has-text-centered">
            <img src="static/images/rm_results.png" alt="Reward Model" width="80%"/>
            <img src="static/images/rm_analysis.png" alt="Reward Model" width="80%"/>
          </div>

          <p>
            <strong>Key findings:</strong>
            <ul>
              <li>Including an extra process reward model to re-rank and select the generated responses benefits a lot after filtering out incorrect responses, even if the reward model itself is not a qualified verifier.</li>
              <li>Our MPRM performs better as a Reranker than as a Verifier (with Answer Filtering).</li>
            </ul>
          </p>
        </div>

        <h2 class="title is-3">Prompt Variation</h2>
        <div class="content has-text-justified">
          <div class="content has-text-centered">
            <img src="static/images/prompt_variation_result.png" alt="Prompt Variation" width="40%"/>
          </div>
          <p>
            <strong>Key findings:</strong> 
            <ul> 
              <li>Adding more unlabeled queries helps only when having perfect reward signals (e.g., the oracle groundtruth answers), and it hurts the performance if the reward model does not generalize well on unseen data.</li>
            </ul>
          </p>
        </div>
        
      </div>
    </div>
  </div>
  
  <section class="hero is-light is-small">
    <div class="hero-body has-text-centered">
      <h1 class="title is-1 mathvista">Dynamics of Self-Evolution</h1>
    </div>
  </section>
  
  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        
          <h2 class="title is-3">Introduction</h2>
          <div class="content has-text-justified">
            <!-- <div class="content has-text-centered">
              <img src="static/images/prompt_variation_result.png" alt="Prompt Variation" width="40%"/>
            </div> -->
            <p>
              We delve even deeper into the current self-evolution strategy to better understand the bottlenecks.
              Instead of analyzing from a design space perspective as previously, we now fix the design parameters and focus exclusively on the training dynamics during the model's self-evolution. 
              This shift in focus allows us to examine the process from an orthogonal angle, providing further insights into the underlying mechanisms that drive or impede progress in multimodal reasoning capabilities.
            </p>
          </div>
        

          <h2 class="title is-3">Monitoring the Training Dynamics</h2>
          <div class="content has-text-justified">
            <p>
              We analyze several key metrics to understand how model changes during the evolution process. 
              <ul>
                <li><b>Greedy Accuracy</b>: the model's accuracy with greedy decoding. We track this metric for reference to compare with other metrics.</li>
                <li><b>Passk@K</b>: the percentage of samples for which the model produces at least one correct response when sampling K candidates. 
                This metric measures the model's exploration ability.</li>
                <li><b>Pass@K ‚Äì Greedy Accuracy</b>: the difference between Pass@K and Greedy accuracy. 
                Typically, Pass@K is an upper bound of Greedy Accuracy, and the gap roughly reflects the percentage of samples where the model,
                while failing in greedy decoding, can generate a correct response when sampling more candidates.
                This gap is crucial for the success of self-evolving training‚Äîa zero gap indicates that the model fails to explore correct
                responses for the current failure cases, suggesting that further training is unlikely to yield significant improvement.</li>
                <li><b>Reward-Pass@2</b>: the percentage of samples for which there exist correct responses among the top 2 responses ranked by the reward model.
                This metric directly reflects the exploitation efficacy of the reward model for the current policy.
                We choose Pass@2 since our training strategy involves selecting the top 2 responses using the reward model</li>
              </ul>
              These metrics serve as a deeper <span style="color:rgb(226, 81, 74)">exploration</span> & <span style="color:rgb(84, 133, 237)">exploitation</span> measure beyond Greedy Accuracy, providing insights into the saturation of exploration
              and help us assess the model's progression towards optimal performance.
            </p>
              <div class="content has-text-centered">
                <img src="static/images/progress_or_regress.png" alt="Progress or Regress" style="width: 40%;" />
              </div>
              <div class="content has-text-centered" style="display: flex; justify-content: center; gap: 10px;">
                  <img src="static/images/passk_diff_temp.png" alt="Pass@K" style="width: 33%;" />
                  <img src="static/images/passk_greedy_gap_diff_temp.png" alt="Pass@K - Greedy Gap" style="width: 33%;" />
                  <img src="static/images/rmpass2_diff_temp.png" alt="RMPass@2" style="width: 33%;" />
              </div>
              <p>
                Exploration saturates during the process of self-evolution especially when the temperature is low. 
                How can we enhance exploration to allow the reward model to exploit more effectively?
              </p>
          </div>

          <h2 class="title is-3">Adaptive Explorations</h2>
          <div class="content has-text-justified">
            <div class="content has-text-centered" style="display: flex; justify-content: center; gap: 10px;">
              <img src="static/images/final_recipe.png" alt="Final Recipe" style="width: 40%;" />
              <img src="static/images/dymanic_two_strategy_v3.png" alt="Dynamic Strategy" style="width: 40%;" />
            </div>
            <ul>
              <li>Monitor <span style="color:rgb(226, 81, 74)">Exploration</span> & <span style="color:rgb(84, 133, 237)">Exploitation</span> during training via validation set</li>
              
              <li>Reward-Pass@2: A Bridge between <span style="color:rgb(226, 81, 74)">Exploration (Generation)</span> & <span style="color:rgb(84, 133, 237)">Exploitation (Rerank)</span></li>
              
              <li>Pass@K: Only considers <span style="color:rgb(226, 81, 74)">Exploration</span> ‚ùå</li>
              
              <li>Performance can be further improved</li>
              
              <li>Alleviate the saturate of self-evolution</li>
              
            </ul>
          </div>
          </div>
        </div>
      </div>
    </div>

<!-- @PAN TODO: bibtex -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre><code>@inproceedings{liu2024mstar,
  author    = {Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  title     = {M-STAR: Multimodal Self-Evolving Training for Reasoning},
  booktitle={International Conference on Learning Representations (ICLR)},
  year      = {2024}
}</code></pre>
  </div>
</section>

<section>
  <div class="section" id="org-banners" style="display:flex">
    <a href="https://www.ucla.edu/" target="_blank" rel="external">
        <img class="center-block org-banner" src="static/images/ucla.png">
    </a>
    <a href="https://www.washington.edu/" target="blank" class="ext-link">
        <img class="center-block org-banner" src="static/images/uw.png">
    </a>
    <a href="https://www.microsoft.com/en-us/research/" target="_blank" rel="external">
        <img class="center-block org-banner" src="static/images/microsoft.png">
    </a>
  </div>
</section>


<footer class="footer">
  <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  <!-- </div> -->
</footer>

</body>
</html>
